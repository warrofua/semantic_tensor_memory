#!/usr/bin/env python3
"""
Test script for Enhanced Concept Analysis

Tests the new concept analysis features to ensure they work with existing S-BERT embeddings.
"""

import sys
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parent / "src"))

import torch
import numpy as np
from memory.universal_core import UniversalMemoryStore
from memory.text_embedder import TextEmbedder
from semantic_tensor_memory.visualization import (
    ConceptAnalyzer,
    analyze_existing_store_concepts,
    visualize_concept_evolution,
)

def test_concept_analysis():
    """Test the enhanced concept analysis pipeline."""
    print("üß† Testing Enhanced Concept Analysis...")
    
    # Create test data with diverse concepts
    test_texts = [
        "Patient exhibits signs of anxiety and stress related to work pressures",
        "Work-related anxiety continues to manifest in sleep disturbances",
        "Discussion of family relationships and childhood experiences",
        "Exploring cognitive behavioral therapy techniques for anxiety management", 
        "Family dynamics continue to influence current relationship patterns",
        "Progress in anxiety management through mindfulness practices",
        "Deep dive into childhood trauma and its lasting effects",
        "Family therapy session focusing on communication patterns",
        "Breakthrough in understanding anxiety triggers and coping mechanisms",
        "Integration of family insights with personal growth goals"
    ]
    
    # Set up Universal STM
    print("üìù Setting up Universal STM with test data...")
    store = UniversalMemoryStore()
    embedder = TextEmbedder()
    
    # Process each text session
    for i, text in enumerate(test_texts):
        embedding = embedder.process_raw_data(text, session_id=f"test_session_{i}")
        store.add_session(embedding)
    
    print(f"‚úÖ Processed {len(store.embeddings)} sessions")
    
    # Test concept analysis
    print("\nüîç Running concept analysis...")
    analyzer = ConceptAnalyzer(store)
    
    # Test individual functions
    print("Testing concept clustering...")
    clusters = analyzer.analyze_concept_clusters(n_clusters=3)
    print(f"Found {len(clusters)} clusters")
    
    for cluster in clusters:
        print(f"  Cluster {cluster.cluster_id}: {len(cluster.session_indices)} sessions")
        print(f"    Keywords: {', '.join(cluster.theme_keywords[:3])}")
        print(f"    Coherence: {cluster.coherence_score:.3f}")
    
    print("\nTesting drift patterns...")
    drift_patterns = analyzer.analyze_concept_drift_patterns()
    print(f"Found {len(drift_patterns)} drift patterns")
    
    for pattern in drift_patterns[:3]:  # Show first 3
        print(f"  Session {pattern.session_from} ‚Üí {pattern.session_to}: {pattern.drift_magnitude:.3f} ({pattern.drift_direction})")
    
    print("\nTesting concept velocity...")
    velocities = analyzer.analyze_concept_velocity()
    if velocities:
        print(f"Velocity range: {min(velocities):.3f} - {max(velocities):.3f}")
        print(f"Average velocity: {np.mean(velocities):.3f}")
    
    print("\nTesting major shifts detection...")
    major_shifts = analyzer.identify_major_concept_shifts(threshold=0.3)
    print(f"Major shifts at sessions: {major_shifts}")
    
    # Test complete analysis
    print("\nüöÄ Running complete concept evolution analysis...")
    evolution = analyze_existing_store_concepts(store, n_clusters=3)
    
    print(f"üìä Analysis Results:")
    print(f"  Total sessions: {evolution.total_sessions}")
    print(f"  Concept clusters: {len(evolution.concept_clusters)}")
    print(f"  Drift patterns: {len(evolution.drift_patterns)}")
    print(f"  Major shifts: {len(evolution.major_shifts)}")
    print(f"  Concept persistence: {len(evolution.concept_persistence)} themes")
    
    # Test visualizations
    print("\nüìä Testing visualizations...")
    viz_types = ["heatmap", "timeline", "velocity", "persistence"]
    
    for viz_type in viz_types:
        try:
            fig = visualize_concept_evolution(evolution, viz_type)
            print(f"  ‚úÖ {viz_type} visualization created successfully")
        except Exception as e:
            print(f"  ‚ùå {viz_type} visualization failed: {str(e)}")
    
    # Test dashboard
    try:
        dashboard_fig = visualize_concept_evolution(evolution, "dashboard")
        print(f"  ‚úÖ Dashboard visualization created successfully")
    except Exception as e:
        print(f"  ‚ùå Dashboard visualization failed: {str(e)}")
    
    print("\nüéâ Concept analysis test completed!")
    return evolution

def test_similarity_matrix():
    """Test the concept similarity matrix functionality."""
    print("\nüîó Testing concept similarity matrix...")
    
    from semantic_tensor_memory.visualization import get_concept_similarity_matrix
    
    # Simple test with a few sessions
    store = UniversalMemoryStore()
    embedder = TextEmbedder()
    
    test_texts = [
        "Anxiety and stress management",
        "Family relationship dynamics", 
        "Childhood trauma processing"
    ]
    
    for i, text in enumerate(test_texts):
        embedding = embedder.process_raw_data(text, session_id=f"sim_test_{i}")
        store.add_session(embedding)
    
    similarity_matrix = get_concept_similarity_matrix(store)
    print(f"Similarity matrix shape: {similarity_matrix.shape}")
    print("Similarity matrix:")
    print(similarity_matrix.round(3))
    
    print("‚úÖ Similarity matrix test completed!")

if __name__ == "__main__":
    try:
        # Run tests
        evolution = test_concept_analysis()
        test_similarity_matrix()
        
        print("\nüéØ All tests passed! Enhanced concept analysis is ready to use.")
        
        # Show some key insights
        print("\nüí° Key Insights from Test:")
        if evolution.concept_clusters:
            largest_cluster = max(evolution.concept_clusters, key=lambda c: len(c.session_indices))
            print(f"  - Largest cluster has {len(largest_cluster.session_indices)} sessions")
            print(f"  - Main themes: {', '.join(largest_cluster.theme_keywords[:3])}")
        
        if evolution.concept_persistence:
            most_persistent = max(evolution.concept_persistence.items(), key=lambda x: x[1])
            print(f"  - Most persistent concept: '{most_persistent[0]}' ({most_persistent[1]:.1%})")
        
        if evolution.major_shifts:
            print(f"  - {len(evolution.major_shifts)} major conceptual shifts detected")
        
    except Exception as e:
        print(f"‚ùå Test failed with error: {str(e)}")
        import traceback
        traceback.print_exc() 